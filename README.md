# Part 1: Artificial Neural Networks (ANN)
## Datasets & Templates:
Artificial-Neural-Networks
## Additional Reading:
<ul>
  <li>
    Yann LeCun et al., 1998, Efficient BackProp
  </li>
  <li>
    By Xavier Glorot et al., 2011 Deep sparse rectifier neural networks
  </li>
  <li>
    CrossValidated, 2015, A list of cost functions used in neural networks, alongside applications

  </li>
    <li>
    Andrew Trask, 2015, A Neural Network in 13 lines of Python (Part 2 – Gradient Descent)

  </li>
    <li>
    Michael Nielsen, 2015, Neural Networks and Deep Learning

  </li>
</ul>

# Part 2: Convolutional Neural Networks (CNN)
## Datasets & Templates:
Convolutional-Neural-Networks
## Additional Reading:
<ul>
  <li>Yann LeCun et al., 1998, Gradient-Based Learning Applied to Document Recognition</li>
  <li>Jianxin Wu, 2017, Introduction to Convolutional Neural Networks</li>
  <li>C.-C. Jay Kuo, 2016, Understanding Convolutional Neural Networks with A Mathematical Model</li>
  <li>Kaiming He et al., 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</li>
  <li>Dominik Scherer et al., 2010, Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition</li>
  <li>Adit Deshpande, 2016, The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)</li>
  <li>Rob DiPietro, 2016, A Friendly Introduction to Cross-Entropy Loss</li>
  <li>Peter Roelants, 2016, How to implement a neural network Intermezzo 2</li>
</ul>


# Part 3: Recurrent Neural Networks (RNN)
## Datasets & Templates:
Recurrent-Neural-Networks
Homework-Challenge
## Additional Reading:
<ul>
  <li>Oscar Sharp & Benjamin, 2016, Sunspring</li>
<li>Sepp (Josef) Hochreiter, 1991, Untersuchungen zu dynamischen neuronalen Netzen</li>
<li>Yoshua Bengio, 1994, Learning Long-Term Dependencies with Gradient Descent is Difficult</li>
<li>Razvan Pascanu, 2013, On the difficulty of training recurrent neural networks</li>
<li>Sepp Hochreiter & Jurgen Schmidhuber, 1997, Long Short-Term Memory</li>
<li>Christopher Olah, 2015, Understanding LSTM Networks</li>
<li>Shi Yan, 2016, Understanding LSTM and its diagrams</li>
<li>Andrej Karpathy, 2015, The Unreasonable Effectiveness of Recurrent Neural Networks</li>
<li>Andrej Karpathy, 2015, Visualizing and Understanding Recurrent Networks</li>
<li>Klaus Greff, 2015, LSTM: A Search Space Odyssey</li>
<li>Xavier Glorot, 2011, Deep sparse rectifier neural networks</li>

</ul>

# Part 4: Self Organizing Maps (SOM)
## Datasets & Templates:
Self-Organizing-Maps
Mega-Case-Study
## Additional Reading:
<ul>
  <li>Tuevo Kohonen, 1990, The Self-Organizing Map</li>
  <li>Mat Buckland, 2004?, Kohonen's Self Organizing Feature Maps</li>
  <li>Nadieh Bremer, 2003, SOM – Creating hexagonal heatmaps with D3.js</li>
</ul>

# Part 5: Boltzmann Machines (BM)
## Datasets & Templates:
Boltzmann-Machines
## Additional Reading:
<ul>
  <li>Yann LeCun, 2006, A Tutorial on Energy-Based Learning</li>
<li>Jaco Van Dormael, 2009, Mr. Nobody</li>
<li>Geoffrey Hinton, 2006, A fast learning algorithm for deep belief nets</li>
<li>Oliver Woodford, 2012?, Notes on Contrastive Divergence</li>
<li>Yoshua Bengio, 2006, Greedy Layer-Wise Training of Deep Networks</li>
<li>Geoffrey Hinton, 1995, The wake-sleep algorithm for unsupervised neural networks</li>
<li>Ruslan Salakhutdinov, 2009?, Deep Boltzmann Machines</li>

</ul>

# Part 6: AutoEncoders (AE)
## Datasets & Templates:
AutoEncoders
## Additional Reading:
<ul>
  <li>Malte Skarupke, 2016, Neural Networks Are Impressively Good At Compression</li>
<li>Francois Chollet, 2016, Building Autoencoders in Keras</li>
<li>Chris McCormick, 2014, Deep Learning Tutorial - Sparse Autoencoder</li>
<li>Eric Wilkinson, 2014, Deep Learning: Sparse Autoencoders</li>
<li>Alireza Makhzani, 2014, k-Sparse Autoencoders</li>
<li>Pascal Vincent, 2008, Extracting and Composing Robust Features with Denoising Autoencoders</li>
<li>Salah Rifai, 2011, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</li>
<li>Pascal Vincent, 2010, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</li>
<li>Geoffrey Hinton, 2006, Reducing the Dimensionality of Data with Neural Networks</li>

</ul>

